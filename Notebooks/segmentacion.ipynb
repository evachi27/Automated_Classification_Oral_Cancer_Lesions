{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rekalantar/MedSegmentAnything_SAM_FineTune/blob/main/MedSegmentAnything_FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uANYZ3YymMMy"
      },
      "source": [
        "**Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_PhzirXlnXl"
      },
      "source": [
        "SegmentAnything (SAM) is an innovative model architecture developed by the Facebook (Meta) research group for generating segmentation masks for a wide range of objects or regions within an image. It's designed to be flexible, capable of segmenting anything from everyday objects to specific structures in medical images. This makes it an ideal tool for many medical imaging tasks.\n",
        "\n",
        "Fine-tuning SAM for medical imaging tasks generally involves a few steps:\n",
        "\n",
        "*   **Loading and Preprocessing the Data:** The first step is to load your medicalimaging data, which often comes in specific formats like DICOM or NIfTI. Libraries such as pydicom or nibabel can be very useful for this. Preprocessing might include tasks such as reorienting the images, normalizing pixel intensities, and converting the images and masks into suitable formats for the model.\n",
        "*   **Creating Bounding Box Prompts:** SAM uses bounding box prompts to guide the segmentation. These bounding boxes should roughly encapsulate the structure you want to segment. You can generate these bounding boxes based on your segmentation masks. Note that SAM accepts multiple bounding boxes, allowing for multi-object segmentation in a single forward pass.\n",
        "\n",
        "*   **Preparing the Model and Processor:** You'll need to load the pre-trained SAM model and its associated processor. The processor is used to prepare your inputs and prompts for the model.\n",
        "\n",
        "*   **Fine-Tuning the Model:** With your data and model ready, you can now fine-tune SAM on your specific task. This often involves running a training loop, computing the loss function (comparing the model's output to the ground truth mask), backpropagating the gradients, and updating the model's weights. SAM is trained to generate segmentation masks that match the ground truth as closely as possible.\n",
        "\n",
        "*   **Evaluating the Model:** After training, you'll want to evaluate your model's performance on a validation set. This will give you an idea of how well your model is likely to perform on unseen data. You could use metrics such as the Dice coefficient or Intersection over Union (IoU) for evaluation.\n",
        "\n",
        "*   **Inference:** With a trained model, you can perform segmentation on new medical images. This involves preparing the image and bounding box prompt, passing them through the model, and post-processing the output to obtain your final segmentation mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csGMcMk1T43P"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1BlkE_DnOWP"
      },
      "outputs": [],
      "source": [
        "!pip install -q monai\n",
        "!pip install -q SimpleITK\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q natsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiAdGVVxkMwH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import errno\n",
        "import glob\n",
        "import monai\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import SimpleITK as sitk\n",
        "from statistics import mean\n",
        "from torch.optim import Adam\n",
        "from natsort import natsorted\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import SamModel,SamConfig\n",
        "import matplotlib.patches as patches\n",
        "from transformers import SamProcessor\n",
        "from IPython.display import clear_output\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.functional import threshold, normalize\n",
        "from skimage.io import imread, imshow\n",
        "import pandas as pd\n",
        "import json\n",
        "import math\n",
        "from skimage import color\n",
        "import cv2\n",
        "import csv\n",
        "from skimage import color\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from monai.transforms import (\n",
        "    EnsureChannelFirstd,\n",
        "    EnsureTyped,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    CopyItemsd,\n",
        "    LoadImaged,\n",
        "    CenterSpatialCropd,\n",
        "    Invertd,\n",
        "    OneOf,\n",
        "    Orientationd,\n",
        "    MapTransform,\n",
        "    NormalizeIntensityd,\n",
        "    RandSpatialCropSamplesd,\n",
        "    CenterSpatialCropd,\n",
        "    RandSpatialCropd,\n",
        "    SpatialPadd,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    RepeatChanneld,\n",
        "    ToTensord,\n",
        "    Resized,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5Td0bOWBEwV"
      },
      "outputs": [],
      "source": [
        "TYPE='_Balanced'\n",
        "#TYPE=\"\"\n",
        "base_dir = './Procesado'+TYPE\n",
        "datasets = ['train', 'valid','test']\n",
        "\n",
        "orig_types = ['images', 'masks']\n",
        "\n",
        "METHOD='SAM' #Method to apply(original image 'SAM' or detection 'SAM+DETR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjXrrRfmmbYq",
        "outputId": "34d57f71-af36-4d2d-fe31-758acd44817f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./Procesado_Balanced/train/images\n",
            "1605\n",
            "./Procesado_Balanced/train/masks\n",
            "1605\n",
            "./Procesado_Balanced/valid/images\n",
            "810\n",
            "./Procesado_Balanced/valid/masks\n",
            "810\n",
            "./Procesado_Balanced/test/images\n",
            "107\n",
            "./Procesado_Balanced/test/masks\n",
            "107\n",
            "Number of training images 1605\n",
            "Number of validation images 810\n",
            "Number of test images 107\n"
          ]
        }
      ],
      "source": [
        "# Initialize dictionary for storing image and label paths\n",
        "data_paths = {}\n",
        "\n",
        "# Create directories and print the number of images and masks in each\n",
        "for dataset in datasets:\n",
        "    folder = 'images'\n",
        "    for data_type in orig_types:\n",
        "        # Construct the directory path\n",
        "        dir_path = os.path.join(base_dir, f'{dataset}/{data_type}')\n",
        "        print(dir_path)\n",
        "\n",
        "        # Find images and labels in the directory\n",
        "        files = sorted(glob.glob(os.path.join(dir_path, \"*.jpg\")))\n",
        "        print(len(files))\n",
        "\n",
        "        # Store the image and label paths in the dictionary\n",
        "        data_paths[f'{dataset}/{data_type}'] = files\n",
        "\n",
        "print('Number of training images', len(data_paths['train/'+folder]))\n",
        "print('Number of validation images', len(data_paths['valid/'+folder]))\n",
        "print('Number of test images', len(data_paths['test/'+folder]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN4RDQuVVcNM"
      },
      "source": [
        "Now we can use a processor instance to prepare the images and prompts for training. The expected image size for the SAM model is 1024x1024 and 3 channels. The target masks are of size 256x256."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567,
          "referenced_widgets": [
            "14caad206f26469aa698ad9e8812c426",
            "67e85c24561c4e9cb987b9016fb75970",
            "d861f1f866eb4f2893dd597b12e96f58",
            "9d181729481b4f6ea82c9652112979e2",
            "84a6d4e6fb524f13b1b3e48ac0f815de",
            "9281f046be544713a828e14c48cc9267",
            "97ade329f27e4116b7ea4aeb0550a8e3",
            "2fbcd6ca7dae441397182cb12aae7aaa",
            "af0e1ea7607d42e686758a234cf1ed06",
            "9e80735ff47447d39d48d2d6da0a3203",
            "c0cd6ba97e324e6c8fc254dad47d780c"
          ]
        },
        "id": "QDazi77-kgz6",
        "outputId": "f61cd3f4-2f30-494b-b131-dadb26af9eff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SamProcessor:\n",
              "- image_processor: SamImageProcessor {\n",
              "  \"do_convert_rgb\": true,\n",
              "  \"do_normalize\": true,\n",
              "  \"do_pad\": true,\n",
              "  \"do_rescale\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"image_mean\": [\n",
              "    0.485,\n",
              "    0.456,\n",
              "    0.406\n",
              "  ],\n",
              "  \"image_processor_type\": \"SamImageProcessor\",\n",
              "  \"image_std\": [\n",
              "    0.229,\n",
              "    0.224,\n",
              "    0.225\n",
              "  ],\n",
              "  \"pad_size\": {\n",
              "    \"height\": 1024,\n",
              "    \"width\": 1024\n",
              "  },\n",
              "  \"processor_class\": \"SamProcessor\",\n",
              "  \"resample\": 2,\n",
              "  \"rescale_factor\": 0.00392156862745098,\n",
              "  \"size\": {\n",
              "    \"longest_edge\": 1024\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create an instance of the processor for image preprocessing\n",
        "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
        "processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE-JlszHkj93"
      },
      "outputs": [],
      "source": [
        "def get_bounding_box(ground_truth_map):\n",
        "    '''\n",
        "    This function creates varying bounding box coordinates based on the segmentation contours as prompt for the SAM model\n",
        "    The padding is random int values between 5 and 20 pixels\n",
        "    '''\n",
        "\n",
        "    if len(np.unique(ground_truth_map)) > 1:\n",
        "\n",
        "        # get bounding box from mask\n",
        "        y_indices, x_indices = np.where(ground_truth_map > 0)\n",
        "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "\n",
        "        # add perturbation to bounding box coordinates\n",
        "        H, W = ground_truth_map.shape\n",
        "        x_min = max(0, x_min - np.random.randint(5, 20))\n",
        "        x_max = min(W, x_max + np.random.randint(5, 20))\n",
        "        y_min = max(0, y_min - np.random.randint(5, 20))\n",
        "        y_max = min(H, y_max + np.random.randint(5, 20))\n",
        "\n",
        "        bbox = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "        return bbox\n",
        "    else:\n",
        "        return [0, 0, 256, 256] # if there is no mask in the array, set bbox to image size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDUcy1BNklym"
      },
      "outputs": [],
      "source": [
        "class SAMDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths, processor):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.processor = processor\n",
        "        self.transforms = transforms = Compose([\n",
        "\n",
        "            # load .jpg files\n",
        "            LoadImaged(keys=['img', 'label']),\n",
        "\n",
        "            # add channel id to match PyTorch configurations\n",
        "            EnsureChannelFirstd(keys=['img', 'label']),\n",
        "\n",
        "            # rescale image and label\n",
        "\n",
        "            Resized(keys=['img'], spatial_size=(1024, 1024)),\n",
        "\n",
        "            Resized(keys=['label'], spatial_size=(256, 256)),\n",
        "\n",
        "            # scale intensities to 0 and 255 to match the expected input intensity range\n",
        "            ScaleIntensityRanged(keys=['img'], a_min=-1000, a_max=2000,\n",
        "                        b_min=0.0, b_max=255.0, clip=True),\n",
        "\n",
        "            ScaleIntensityRanged(keys=['label'], a_min=0, a_max=255,\n",
        "                         b_min=0.0, b_max=1.0, clip=True),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "\n",
        "        # create a dict of images and labels to apply Monai's dictionary transforms\n",
        "        data_dict = self.transforms({'img': image_path, 'label': mask_path})\n",
        "\n",
        "        # squeeze extra dimensions\n",
        "        image = data_dict['img'].squeeze()\n",
        "        ground_truth_mask = data_dict['label'].squeeze()\n",
        "        weights = np.array([0.2989, 0.5870, 0.1140])\n",
        "        ground_truth_mask = np.sum(ground_truth_mask * weights[:, np.newaxis, np.newaxis], axis=0)\n",
        "\n",
        "        # convert to int type for huggingface's models expected inputs\n",
        "        image_rgb = image.astype(np.uint8)\n",
        "\n",
        "        mask = np.zeros([256, 256])\n",
        "        ind=(ground_truth_mask>0.1)\n",
        "        mask[ind]=1\n",
        "\n",
        "        prompt = get_bounding_box(mask)\n",
        "\n",
        "        # prepare image and prompt for the model\n",
        "        inputs = self.processor(image_rgb, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
        "\n",
        "        # remove batch dimension which the processor adds by default\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "\n",
        "        # add ground truth segmentation (ground truth image size is 256x256)\n",
        "        inputs[\"ground_truth_mask\"] = torch.from_numpy(mask.astype(np.int8))\n",
        "        inputs[\"path_name\"]=image_path\n",
        "\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O79mlP4tknaC"
      },
      "outputs": [],
      "source": [
        "# create train and validation dataloaders\n",
        "train_dataset = SAMDataset(image_paths=data_paths['train/'+folder], mask_paths=data_paths['train/masks'], processor=processor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "val_dataset = SAMDataset(image_paths=data_paths['valid/'+folder], mask_paths=data_paths['valid/masks'], processor=processor)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW9yndttWjqP"
      },
      "source": [
        "Finally, we can visualize our processed data along with the bounding boxes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5eiGlkwkpGq"
      },
      "outputs": [],
      "source": [
        "example = train_dataset[1]\n",
        "for k,v in example.items():\n",
        "    if type(k) != str:\n",
        "        print(k,v.shape)\n",
        "\n",
        "xmin, ymin, xmax, ymax = get_bounding_box(example['ground_truth_mask'])\n",
        "\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "\n",
        "axs[0].imshow(example['pixel_values'][1], cmap='gray')\n",
        "axs[0].axis('off')\n",
        "\n",
        "axs[1].imshow(example['ground_truth_mask'], cmap='copper')\n",
        "\n",
        "# create a Rectangle patch for the bounding box\n",
        "rect = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
        "\n",
        "# add the patch to the second Axes\n",
        "axs[1].add_patch(rect)\n",
        "\n",
        "axs[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wtL8tHfXAOg"
      },
      "source": [
        "In order to finetune the model, we freeze the encoder weights from the pre-trained SAM model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUS6s5NJkqBv",
        "outputId": "94e20876-640a-49e6-9589-90f8c54d359f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vision_encoder.pos_embed\n",
            "vision_encoder.patch_embed.projection.weight\n",
            "vision_encoder.patch_embed.projection.bias\n",
            "vision_encoder.layers.0.layer_norm1.weight\n",
            "vision_encoder.layers.0.layer_norm1.bias\n",
            "vision_encoder.layers.0.attn.rel_pos_h\n",
            "vision_encoder.layers.0.attn.rel_pos_w\n",
            "vision_encoder.layers.0.attn.qkv.weight\n",
            "vision_encoder.layers.0.attn.qkv.bias\n",
            "vision_encoder.layers.0.attn.proj.weight\n",
            "vision_encoder.layers.0.attn.proj.bias\n",
            "vision_encoder.layers.0.layer_norm2.weight\n",
            "vision_encoder.layers.0.layer_norm2.bias\n",
            "vision_encoder.layers.0.mlp.lin1.weight\n",
            "vision_encoder.layers.0.mlp.lin1.bias\n",
            "vision_encoder.layers.0.mlp.lin2.weight\n",
            "vision_encoder.layers.0.mlp.lin2.bias\n",
            "vision_encoder.layers.1.layer_norm1.weight\n",
            "vision_encoder.layers.1.layer_norm1.bias\n",
            "vision_encoder.layers.1.attn.rel_pos_h\n",
            "vision_encoder.layers.1.attn.rel_pos_w\n",
            "vision_encoder.layers.1.attn.qkv.weight\n",
            "vision_encoder.layers.1.attn.qkv.bias\n",
            "vision_encoder.layers.1.attn.proj.weight\n",
            "vision_encoder.layers.1.attn.proj.bias\n",
            "vision_encoder.layers.1.layer_norm2.weight\n",
            "vision_encoder.layers.1.layer_norm2.bias\n",
            "vision_encoder.layers.1.mlp.lin1.weight\n",
            "vision_encoder.layers.1.mlp.lin1.bias\n",
            "vision_encoder.layers.1.mlp.lin2.weight\n",
            "vision_encoder.layers.1.mlp.lin2.bias\n",
            "vision_encoder.layers.2.layer_norm1.weight\n",
            "vision_encoder.layers.2.layer_norm1.bias\n",
            "vision_encoder.layers.2.attn.rel_pos_h\n",
            "vision_encoder.layers.2.attn.rel_pos_w\n",
            "vision_encoder.layers.2.attn.qkv.weight\n",
            "vision_encoder.layers.2.attn.qkv.bias\n",
            "vision_encoder.layers.2.attn.proj.weight\n",
            "vision_encoder.layers.2.attn.proj.bias\n",
            "vision_encoder.layers.2.layer_norm2.weight\n",
            "vision_encoder.layers.2.layer_norm2.bias\n",
            "vision_encoder.layers.2.mlp.lin1.weight\n",
            "vision_encoder.layers.2.mlp.lin1.bias\n",
            "vision_encoder.layers.2.mlp.lin2.weight\n",
            "vision_encoder.layers.2.mlp.lin2.bias\n",
            "vision_encoder.layers.3.layer_norm1.weight\n",
            "vision_encoder.layers.3.layer_norm1.bias\n",
            "vision_encoder.layers.3.attn.rel_pos_h\n",
            "vision_encoder.layers.3.attn.rel_pos_w\n",
            "vision_encoder.layers.3.attn.qkv.weight\n",
            "vision_encoder.layers.3.attn.qkv.bias\n",
            "vision_encoder.layers.3.attn.proj.weight\n",
            "vision_encoder.layers.3.attn.proj.bias\n",
            "vision_encoder.layers.3.layer_norm2.weight\n",
            "vision_encoder.layers.3.layer_norm2.bias\n",
            "vision_encoder.layers.3.mlp.lin1.weight\n",
            "vision_encoder.layers.3.mlp.lin1.bias\n",
            "vision_encoder.layers.3.mlp.lin2.weight\n",
            "vision_encoder.layers.3.mlp.lin2.bias\n",
            "vision_encoder.layers.4.layer_norm1.weight\n",
            "vision_encoder.layers.4.layer_norm1.bias\n",
            "vision_encoder.layers.4.attn.rel_pos_h\n",
            "vision_encoder.layers.4.attn.rel_pos_w\n",
            "vision_encoder.layers.4.attn.qkv.weight\n",
            "vision_encoder.layers.4.attn.qkv.bias\n",
            "vision_encoder.layers.4.attn.proj.weight\n",
            "vision_encoder.layers.4.attn.proj.bias\n",
            "vision_encoder.layers.4.layer_norm2.weight\n",
            "vision_encoder.layers.4.layer_norm2.bias\n",
            "vision_encoder.layers.4.mlp.lin1.weight\n",
            "vision_encoder.layers.4.mlp.lin1.bias\n",
            "vision_encoder.layers.4.mlp.lin2.weight\n",
            "vision_encoder.layers.4.mlp.lin2.bias\n",
            "vision_encoder.layers.5.layer_norm1.weight\n",
            "vision_encoder.layers.5.layer_norm1.bias\n",
            "vision_encoder.layers.5.attn.rel_pos_h\n",
            "vision_encoder.layers.5.attn.rel_pos_w\n",
            "vision_encoder.layers.5.attn.qkv.weight\n",
            "vision_encoder.layers.5.attn.qkv.bias\n",
            "vision_encoder.layers.5.attn.proj.weight\n",
            "vision_encoder.layers.5.attn.proj.bias\n",
            "vision_encoder.layers.5.layer_norm2.weight\n",
            "vision_encoder.layers.5.layer_norm2.bias\n",
            "vision_encoder.layers.5.mlp.lin1.weight\n",
            "vision_encoder.layers.5.mlp.lin1.bias\n",
            "vision_encoder.layers.5.mlp.lin2.weight\n",
            "vision_encoder.layers.5.mlp.lin2.bias\n",
            "vision_encoder.layers.6.layer_norm1.weight\n",
            "vision_encoder.layers.6.layer_norm1.bias\n",
            "vision_encoder.layers.6.attn.rel_pos_h\n",
            "vision_encoder.layers.6.attn.rel_pos_w\n",
            "vision_encoder.layers.6.attn.qkv.weight\n",
            "vision_encoder.layers.6.attn.qkv.bias\n",
            "vision_encoder.layers.6.attn.proj.weight\n",
            "vision_encoder.layers.6.attn.proj.bias\n",
            "vision_encoder.layers.6.layer_norm2.weight\n",
            "vision_encoder.layers.6.layer_norm2.bias\n",
            "vision_encoder.layers.6.mlp.lin1.weight\n",
            "vision_encoder.layers.6.mlp.lin1.bias\n",
            "vision_encoder.layers.6.mlp.lin2.weight\n",
            "vision_encoder.layers.6.mlp.lin2.bias\n",
            "vision_encoder.layers.7.layer_norm1.weight\n",
            "vision_encoder.layers.7.layer_norm1.bias\n",
            "vision_encoder.layers.7.attn.rel_pos_h\n",
            "vision_encoder.layers.7.attn.rel_pos_w\n",
            "vision_encoder.layers.7.attn.qkv.weight\n",
            "vision_encoder.layers.7.attn.qkv.bias\n",
            "vision_encoder.layers.7.attn.proj.weight\n",
            "vision_encoder.layers.7.attn.proj.bias\n",
            "vision_encoder.layers.7.layer_norm2.weight\n",
            "vision_encoder.layers.7.layer_norm2.bias\n",
            "vision_encoder.layers.7.mlp.lin1.weight\n",
            "vision_encoder.layers.7.mlp.lin1.bias\n",
            "vision_encoder.layers.7.mlp.lin2.weight\n",
            "vision_encoder.layers.7.mlp.lin2.bias\n",
            "vision_encoder.layers.8.layer_norm1.weight\n",
            "vision_encoder.layers.8.layer_norm1.bias\n",
            "vision_encoder.layers.8.attn.rel_pos_h\n",
            "vision_encoder.layers.8.attn.rel_pos_w\n",
            "vision_encoder.layers.8.attn.qkv.weight\n",
            "vision_encoder.layers.8.attn.qkv.bias\n",
            "vision_encoder.layers.8.attn.proj.weight\n",
            "vision_encoder.layers.8.attn.proj.bias\n",
            "vision_encoder.layers.8.layer_norm2.weight\n",
            "vision_encoder.layers.8.layer_norm2.bias\n",
            "vision_encoder.layers.8.mlp.lin1.weight\n",
            "vision_encoder.layers.8.mlp.lin1.bias\n",
            "vision_encoder.layers.8.mlp.lin2.weight\n",
            "vision_encoder.layers.8.mlp.lin2.bias\n",
            "vision_encoder.layers.9.layer_norm1.weight\n",
            "vision_encoder.layers.9.layer_norm1.bias\n",
            "vision_encoder.layers.9.attn.rel_pos_h\n",
            "vision_encoder.layers.9.attn.rel_pos_w\n",
            "vision_encoder.layers.9.attn.qkv.weight\n",
            "vision_encoder.layers.9.attn.qkv.bias\n",
            "vision_encoder.layers.9.attn.proj.weight\n",
            "vision_encoder.layers.9.attn.proj.bias\n",
            "vision_encoder.layers.9.layer_norm2.weight\n",
            "vision_encoder.layers.9.layer_norm2.bias\n",
            "vision_encoder.layers.9.mlp.lin1.weight\n",
            "vision_encoder.layers.9.mlp.lin1.bias\n",
            "vision_encoder.layers.9.mlp.lin2.weight\n",
            "vision_encoder.layers.9.mlp.lin2.bias\n",
            "vision_encoder.layers.10.layer_norm1.weight\n",
            "vision_encoder.layers.10.layer_norm1.bias\n",
            "vision_encoder.layers.10.attn.rel_pos_h\n",
            "vision_encoder.layers.10.attn.rel_pos_w\n",
            "vision_encoder.layers.10.attn.qkv.weight\n",
            "vision_encoder.layers.10.attn.qkv.bias\n",
            "vision_encoder.layers.10.attn.proj.weight\n",
            "vision_encoder.layers.10.attn.proj.bias\n",
            "vision_encoder.layers.10.layer_norm2.weight\n",
            "vision_encoder.layers.10.layer_norm2.bias\n",
            "vision_encoder.layers.10.mlp.lin1.weight\n",
            "vision_encoder.layers.10.mlp.lin1.bias\n",
            "vision_encoder.layers.10.mlp.lin2.weight\n",
            "vision_encoder.layers.10.mlp.lin2.bias\n",
            "vision_encoder.layers.11.layer_norm1.weight\n",
            "vision_encoder.layers.11.layer_norm1.bias\n",
            "vision_encoder.layers.11.attn.rel_pos_h\n",
            "vision_encoder.layers.11.attn.rel_pos_w\n",
            "vision_encoder.layers.11.attn.qkv.weight\n",
            "vision_encoder.layers.11.attn.qkv.bias\n",
            "vision_encoder.layers.11.attn.proj.weight\n",
            "vision_encoder.layers.11.attn.proj.bias\n",
            "vision_encoder.layers.11.layer_norm2.weight\n",
            "vision_encoder.layers.11.layer_norm2.bias\n",
            "vision_encoder.layers.11.mlp.lin1.weight\n",
            "vision_encoder.layers.11.mlp.lin1.bias\n",
            "vision_encoder.layers.11.mlp.lin2.weight\n",
            "vision_encoder.layers.11.mlp.lin2.bias\n",
            "vision_encoder.neck.conv1.weight\n",
            "vision_encoder.neck.layer_norm1.weight\n",
            "vision_encoder.neck.layer_norm1.bias\n",
            "vision_encoder.neck.conv2.weight\n",
            "vision_encoder.neck.layer_norm2.weight\n",
            "vision_encoder.neck.layer_norm2.bias\n",
            "prompt_encoder.mask_embed.conv1.weight\n",
            "prompt_encoder.mask_embed.conv1.bias\n",
            "prompt_encoder.mask_embed.conv2.weight\n",
            "prompt_encoder.mask_embed.conv2.bias\n",
            "prompt_encoder.mask_embed.conv3.weight\n",
            "prompt_encoder.mask_embed.conv3.bias\n",
            "prompt_encoder.mask_embed.layer_norm1.weight\n",
            "prompt_encoder.mask_embed.layer_norm1.bias\n",
            "prompt_encoder.mask_embed.layer_norm2.weight\n",
            "prompt_encoder.mask_embed.layer_norm2.bias\n",
            "prompt_encoder.no_mask_embed.weight\n",
            "prompt_encoder.point_embed.0.weight\n",
            "prompt_encoder.point_embed.1.weight\n",
            "prompt_encoder.point_embed.2.weight\n",
            "prompt_encoder.point_embed.3.weight\n",
            "prompt_encoder.not_a_point_embed.weight\n"
          ]
        }
      ],
      "source": [
        "# load the pretrained weights for finetuning\n",
        "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
        "\n",
        "# make sure we only compute gradients for mask decoder (encoder weights are frozen)\n",
        "for name, param in model.named_parameters():\n",
        "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "        print(name)\n",
        "        param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tmeA4b6BEwb"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuQGNi6UXaDP"
      },
      "source": [
        "**Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZfShMWFBEwb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    os.makedirs('./SAMResults')\n",
        "    os.makedirs('./SAMResults/best_models')\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AmFx1Ibkrtz"
      },
      "outputs": [],
      "source": [
        "inicio = time.time()\n",
        "# define training loop\n",
        "num_epochs = 100\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# define optimizer\n",
        "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
        "\n",
        "# define segmentation loss with sigmoid activation applied to predictions from the model\n",
        "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
        "\n",
        "# track mean train and validation losses\n",
        "mean_train_losses, mean_val_losses = [], []\n",
        "\n",
        "# create an artibarily large starting validation loss value\n",
        "best_val_loss = 100.0\n",
        "best_val_epoch = 0\n",
        "\n",
        "# set model to train mode for gradient updating\n",
        "model.train()\n",
        "for epoch in tqdm(range(num_epochs),desc=\"Épocas\"):\n",
        "\n",
        "    # create temporary list to record training losses\n",
        "    epoch_losses = []\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # forward pass\n",
        "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                      input_boxes=batch[\"input_boxes\"].to(device),\n",
        "                      multimask_output=False)\n",
        "\n",
        "        # compute loss\n",
        "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
        "\n",
        "        # backward pass (compute gradients of parameters w.r.t. loss)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # optimize\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "        # visualize training predictions every 50 iterations\n",
        "        if i % 50 == 0:\n",
        "\n",
        "            # clear jupyter cell output\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            fig, axs = plt.subplots(1, 3)\n",
        "            xmin, ymin, xmax, ymax = get_bounding_box(batch['ground_truth_mask'][0])\n",
        "            rect = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, linewidth=1, edgecolor='r',\n",
        "                                     facecolor='none')\n",
        "\n",
        "            axs[0].set_title('input image')\n",
        "            axs[0].imshow(batch[\"pixel_values\"][0,1], cmap='gray')\n",
        "            axs[0].axis('off')\n",
        "\n",
        "            axs[1].set_title('ground truth mask')\n",
        "            axs[1].imshow(batch['ground_truth_mask'][0], cmap='copper')\n",
        "            axs[1].add_patch(rect)\n",
        "            axs[1].axis('off')\n",
        "\n",
        "            # apply sigmoid\n",
        "            medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "\n",
        "            # convert soft mask to hard mask\n",
        "            medsam_seg_prob = medsam_seg_prob.detach().cpu().numpy().squeeze()\n",
        "            medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "            axs[2].set_title('predicted mask')\n",
        "            axs[2].imshow(medsam_seg, cmap='copper')\n",
        "            axs[2].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # create temporary list to record validation losses\n",
        "    val_losses = []\n",
        "\n",
        "    # set model to eval mode for validation\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_dataloader:\n",
        "\n",
        "            # forward pass\n",
        "            outputs = model(pixel_values=val_batch[\"pixel_values\"].to(device),\n",
        "                      input_boxes=val_batch[\"input_boxes\"].to(device),\n",
        "                      multimask_output=False)\n",
        "\n",
        "            # calculate val loss\n",
        "            predicted_val_masks = outputs.pred_masks.squeeze(1)\n",
        "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "            val_loss = seg_loss(predicted_val_masks, ground_truth_masks.unsqueeze(1))\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "        # visualize the last validation prediction\n",
        "        fig, axs = plt.subplots(1, 3)\n",
        "        xmin, ymin, xmax, ymax = get_bounding_box(val_batch['ground_truth_mask'][0])\n",
        "\n",
        "        axs[0].set_title('input image')\n",
        "        axs[0].imshow(val_batch[\"pixel_values\"][0,1], cmap='gray')\n",
        "        axs[0].axis('off')\n",
        "\n",
        "        axs[1].set_title('ground truth mask')\n",
        "        axs[1].imshow(val_batch['ground_truth_mask'][0], cmap='copper')\n",
        "        #axs[1].add_patch(rect)\n",
        "        axs[1].axis('off')\n",
        "\n",
        "        # apply sigmoid\n",
        "        medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "\n",
        "        # convert soft mask to hard mask\n",
        "        medsam_seg_prob = medsam_seg_prob.detach().cpu().numpy().squeeze()\n",
        "        medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "        axs[2].set_title('predicted mask')\n",
        "        axs[2].imshow(medsam_seg, cmap='copper')\n",
        "        axs[2].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # save the best weights and record the best performing epoch\n",
        "        if mean(val_losses) < best_val_loss:\n",
        "            torch.save(model.state_dict(), f\"./SAMResults/best_models/best_weights2_balanced.pth\")\n",
        "            print(f\"Model Was Saved! Current Best val loss {best_val_loss}\")\n",
        "            best_val_loss = mean(val_losses)\n",
        "            best_val_epoch = epoch\n",
        "        else:\n",
        "            print(\"Model Was Not Saved!\")\n",
        "\n",
        "    print(f'EPOCH: {epoch}')\n",
        "    print(f'Mean loss: {mean(epoch_losses)}')\n",
        "\n",
        "    mean_train_losses.append(mean(epoch_losses))\n",
        "    mean_val_losses.append(mean(val_losses))\n",
        "print(\"Entrenado en: \", round((time.time()-inicio)/60,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaasRyRiBEwc",
        "outputId": "87cd8a0e-013c-48ba-a4b0-56046663db96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.1893078379541914, 0.16077119806473872, 0.155571824702147, 0.15071400118022693, 0.1454905291212682, 0.13960010919986857, 0.1346950808046763, 0.1299981718494142, 0.12471443125020677, 0.12143349937189406, 0.11559977757967893, 0.1122417347453465, 0.1089594795325092, 0.10460114798441854, 0.1024728653215545, 0.09937414762758391, 0.09680039522432464, 0.09484490148746336, 0.09201044512686328, 0.08987316869872382, 0.08719571334922054, 0.08442066755621604, 0.0824760456322881, 0.08159185543981297, 0.08002666517210155, 0.07749425934111218, 0.07543943819598617, 0.07389088865381163, 0.07268091715013499, 0.07171095917900774, 0.0694904035868303, 0.0685374611634703, 0.06715506861143022, 0.06623564824879727, 0.06463676549935267, 0.06356901966522788, 0.06382684807911097, 0.06070357074618711, 0.06067648297901094, 0.05877307835397691, 0.05883645651125091, 0.05794646936785024, 0.0565747282958105, 0.05674248101926667, 0.054956769126226594, 0.05385955601837776, 0.05330570319731288, 0.054379924509755546, 0.051469816337121985, 0.05054149070632792, 0.051348141532077965, 0.04981426836174225, 0.049287627344933624, 0.04887287360485469, 0.04811487929471928, 0.047195638080252296, 0.0468904513064946, 0.04604082404638748, 0.046065195512920154, 0.04515202970148247, 0.04535259966538331, 0.04423684271696572, 0.04392419890079914, 0.04262816088964635, 0.04282713504595177, 0.042063359568052204, 0.04153808500165137, 0.04170097302053576, 0.04067945391218239, 0.04065532543206141, 0.04037565509106883, 0.03982836516846749, 0.03920105833130833, 0.039394873510639986, 0.03884391710394268, 0.037691441131900774, 0.03830610164600741, 0.037307735618401165, 0.0373081923273865, 0.0366993614446337, 0.03734793781862823, 0.036174055237636386, 0.03644458813087963, 0.03554002020589288, 0.03584630912709459, 0.03505503793371801, 0.03482079936707874, 0.034629487545690806, 0.03435364560546162, 0.034656036085800215, 0.03345859444401346, 0.03364115530085341, 0.033414294043805366, 0.0329453424129902, 0.03309497710700347, 0.03240583819392314, 0.03214939260779883, 0.031899287953183655, 0.03239641412396297, 0.03133777350280144]\n",
            "[0.8711582256688012, 0.4612959975813642, 0.8110549353523019, 0.5291154086589813, 0.48673405161610356, 0.7339731468830579, 0.6962826532346231, 0.5335500404422666, 0.609973618277797, 0.5531217891492961, 0.5485433958930733, 0.7594714223602672, 0.5756170247807915, 0.7954859667354159, 0.8744284069832461, 0.5921351474008443, 0.7790661582976212, 0.6277161228068081, 0.5759012318687674, 0.6282917043309153, 0.5577028890450796, 0.5518910899574374, 0.5961198384379163, 0.46326939161912895, 0.5425756761321315, 0.7751666520848687, 0.588532602124744, 0.7268228558110602, 0.5140140721091517, 0.6448083575860953, 0.6607552379001805, 0.5977287711184702, 0.8490660923498649, 0.5784440517425538, 0.5272756595670441, 0.6944107660540828, 0.68095538697125, 0.6575411949628666, 0.6371462085364777, 0.9282819098160591, 0.5125195378874555, 0.8233888138959438, 0.6222876966735463, 0.7006098056281055, 0.8067627294563953, 0.6447568267951777, 0.65704196124901, 0.5619979561846933, 0.8805332964585151, 0.6566070248315363, 0.6042932280787715, 0.6445352751531719, 0.620694492039857, 0.7662153960010152, 0.6608812165113143, 0.594659631119834, 0.5874266233709123, 0.6557368151199671, 0.5879358974503882, 0.8063084343333303, 0.5206649643403513, 0.6472451452119851, 0.5335341380702124, 0.546733326970795, 0.6486500310309139, 0.9052853992691746, 0.643898445809329, 0.6420898351404402, 0.6438429811118561, 0.7575001944730311, 0.8066578672255998, 0.637501127705162, 0.5292067652867164, 0.5837391650235212, 0.5421026955416173, 0.6478251801596747, 0.551345792155207, 0.6156618829862571, 0.5571442613631119, 0.8610254436363408, 0.7399452251416666, 0.8858867719585513, 0.6811878397876834, 0.5671988111955149, 0.5523490484114046, 0.7238634770299182, 0.7363080182929098, 0.918756412576746, 0.7472268174459905, 0.6469682914975249, 0.8041232366620759, 0.6071248203148076, 0.4634169207678901, 0.6155889246934726, 0.7733063237166699, 0.5688797018410247, 0.8859218883661576, 0.7589203431282514, 0.690686892506517, 0.8078043246710742]\n"
          ]
        }
      ],
      "source": [
        "print(mean_train_losses)\n",
        "print(mean_val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8Y3__pvBEwc"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = './SAMResults/'+METHOD+TYPE\n",
        "model.save_pretrained(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8jjspcDBEwd",
        "outputId": "6de3e316-fc03-4b19-8479-55a077566e17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SamModel(\n",
              "  (shared_image_embedding): SamPositionalEmbedding()\n",
              "  (vision_encoder): SamVisionEncoder(\n",
              "    (patch_embed): SamPatchEmbeddings(\n",
              "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x SamVisionLayer(\n",
              "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): SamVisionAttention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): SamMLPBlock(\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (neck): SamVisionNeck(\n",
              "      (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (layer_norm1): SamLayerNorm()\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (layer_norm2): SamLayerNorm()\n",
              "    )\n",
              "  )\n",
              "  (prompt_encoder): SamPromptEncoder(\n",
              "    (shared_embedding): SamPositionalEmbedding()\n",
              "    (mask_embed): SamMaskEmbedding(\n",
              "      (activation): GELUActivation()\n",
              "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (layer_norm1): SamLayerNorm()\n",
              "      (layer_norm2): SamLayerNorm()\n",
              "    )\n",
              "    (no_mask_embed): Embedding(1, 256)\n",
              "    (point_embed): ModuleList(\n",
              "      (0-3): 4 x Embedding(1, 256)\n",
              "    )\n",
              "    (not_a_point_embed): Embedding(1, 256)\n",
              "  )\n",
              "  (mask_decoder): SamMaskDecoder(\n",
              "    (iou_token): Embedding(1, 256)\n",
              "    (mask_tokens): Embedding(4, 256)\n",
              "    (transformer): SamTwoWayTransformer(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
              "          (self_attn): SamAttention(\n",
              "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (cross_attn_token_to_image): SamAttention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): SamMLPBlock(\n",
              "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (act): ReLU()\n",
              "          )\n",
              "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (cross_attn_image_to_token): SamAttention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_attn_token_to_image): SamAttention(\n",
              "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "      )\n",
              "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "    (upscale_layer_norm): SamLayerNorm()\n",
              "    (activation): GELU(approximate='none')\n",
              "    (output_hypernetworks_mlps): ModuleList(\n",
              "      (0-3): 4 x SamFeedForward(\n",
              "        (activation): ReLU()\n",
              "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
              "        (layers): ModuleList(\n",
              "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (iou_prediction_head): SamFeedForward(\n",
              "      (activation): ReLU()\n",
              "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the model\n",
        "modelo = SamModel(SamConfig(num_labels=2))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "modelo.to(device)\n",
        "# Specifies the path to the. pth file of the model\n",
        "ruta_del_archivo = \"./SAMResults/best_models/best_weights1_balanced.pth\"\n",
        "\n",
        "# Load the weights trained on the model\n",
        "modelo.load_state_dict(torch.load(ruta_del_archivo,map_location=torch.device('cpu')))\n",
        "\n",
        "# Set the model in evaluation mode (no training)\n",
        "modelo.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fduQfdXVBEwd"
      },
      "outputs": [],
      "source": [
        "def dice_coefficient(mask1, mask2):\n",
        "    intersection = np.logical_and(mask1, mask2)\n",
        "    dice_val = (2.0 * intersection.sum()) / (mask1.sum() + mask2.sum())\n",
        "    return round(dice_val,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XuJY6MDBEwd"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data_Balanced/DETR/resultados.csv')\n",
        "df = df.fillna('NULL')\n",
        "bboxes = [] #DETR\n",
        "bboxes_gold = [] #GOLD\n",
        "img_nombres = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    # Get the image name and prediction (if no prediction is taken the entire image)\n",
        "    nombre_imagen = row['imagen']\n",
        "    #DETR\n",
        "    if row['predicion'] != 'NULL' :\n",
        "        prediccion = [round(json.loads(row['predicion'])[0],0),\n",
        "                      round(json.loads(row['predicion'])[1],0),\n",
        "                      round(json.loads(row['predicion'])[2],0),\n",
        "                      round(json.loads(row['predicion'])[3],0)]\n",
        "\n",
        "    else:\n",
        "        prediccion = [0, 0, 256, 256]\n",
        "    #GOLD\n",
        "    g_xmin = json.loads(row['true_box'])[0]\n",
        "    g_ymin = json.loads(row['true_box'])[1]\n",
        "    g_xmax = json.loads(row['true_box'])[2] + g_xmin\n",
        "    g_ymax = json.loads(row['true_box'])[3] + g_ymin\n",
        "\n",
        "    img_nombres.append(nombre_imagen)\n",
        "    bboxes_gold.append([g_xmin,g_ymin,g_xmax,g_ymax])\n",
        "    bboxes.append(prediccion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W76to6T3BEwd"
      },
      "outputs": [],
      "source": [
        "bbox_dict=dict(zip(img_nombres,bboxes))\n",
        "bbox_gold_dict=dict(zip(img_nombres,bboxes_gold))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fOOmtxUBEwe",
        "outputId": "b6514e7b-f6a9-44dd-b2d3-a0c2eac65374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[137.0, 62.0, 189.0, 115.0] \n",
            " [65, 54, 109, 165]\n"
          ]
        }
      ],
      "source": [
        "print(bbox_dict.get('00015C.jpg'),'\\n',bbox_gold_dict.get('00015C.jpg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz8POpboBEwe"
      },
      "outputs": [],
      "source": [
        "#Function to obtain the bbox obtained by DETR for the SAM+DETR set and pass it as input_boxes\n",
        "def get_boxfromDETR(name):\n",
        "    return bbox_dict.get(name)\n",
        "\n",
        "#Function to obtain the bbox for the SAM set and pass it as input_boxes\n",
        "def get_goldbox(name):\n",
        "    return bbox_gold_dict.get(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po-KYoQqBEwe"
      },
      "outputs": [],
      "source": [
        "class SAM_Pred_Dataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths, processor):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.processor = processor\n",
        "        self.transforms = transforms = Compose([\n",
        "\n",
        "            # load .jpg files\n",
        "            LoadImaged(keys=['img', 'label']),\n",
        "            # add channel id to match PyTorch configurations\n",
        "            EnsureChannelFirstd(keys=['img', 'label']),\n",
        "            Resized(keys=['img'], spatial_size=(1024, 1024)),\n",
        "            Resized(keys=['label'], spatial_size=(256, 256)),\n",
        "            # scale intensities to 0 and 255 to match the expected input intensity range\n",
        "            ScaleIntensityRanged(keys=['img'], a_min=-1000, a_max=2000,\n",
        "                        b_min=0.0, b_max=255.0, clip=True),\n",
        "            ScaleIntensityRanged(keys=['label'], a_min=0, a_max=255,\n",
        "                         b_min=0.0, b_max=1.0, clip=True)\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "\n",
        "        data_dict = self.transforms({'img': image_path, 'label': mask_path})\n",
        "\n",
        "        image = data_dict['img'].squeeze()\n",
        "        ground_truth_mask = data_dict['label'].squeeze()\n",
        "        weights = np.array([0.2989, 0.5870, 0.1140])\n",
        "        ground_truth_mask = np.sum(ground_truth_mask * weights[:, np.newaxis, np.newaxis], axis=0)\n",
        "        image_rgb = image.astype(np.uint8)\n",
        "\n",
        "        mask = np.zeros([256, 256])\n",
        "        ind=(ground_truth_mask>0.1)\n",
        "        mask[ind]=1\n",
        "\n",
        "        name = image_path.split('/')[-1]\n",
        "        if METHOD == 'SAM':\n",
        "            prompt = get_goldbox(name)\n",
        "        else:\n",
        "            prompt = get_boxfromDETR(name)\n",
        "\n",
        "        # prepare image and prompt for the model\n",
        "        inputs = self.processor(image_rgb, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
        "\n",
        "        # remove batch dimension which the processor adds by default\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "\n",
        "        # add ground truth segmentation (ground truth image size is 256x256)\n",
        "        inputs[\"ground_truth_mask\"] = torch.from_numpy(mask.astype(np.int8))\n",
        "\n",
        "        inputs[\"path_name\"]=image_path\n",
        "\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xunoXkPkSiyo"
      },
      "outputs": [],
      "source": [
        "#GOLD bbox\n",
        "METHOD ='SAM'\n",
        "test_dataset = SAM_Pred_Dataset(image_paths=data_paths['test/'+folder],\n",
        "                                mask_paths=data_paths['test/masks'],\n",
        "                                processor=processor)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWL6nmc9BEwf",
        "outputId": "b29d75c7-6ae8-4b4e-9d76-daa1cceaf5ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['./Procesado_Balanced/test/images/00015C.jpg']\n",
            "[65.0, 54.0, 109.0, 165.0]\n"
          ]
        }
      ],
      "source": [
        "for n,batch in enumerate(test_dataloader):\n",
        "    if n == 14:\n",
        "        print(batch['path_name'])\n",
        "        print(batch['input_boxes'][0][0].tolist())\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoctDtIgBEwg"
      },
      "outputs": [],
      "source": [
        "#DETR bbox\n",
        "METHOD ='SAM+DETR'\n",
        "test_dataset2 = SAM_Pred_Dataset(image_paths=data_paths['test/'+folder],\n",
        "                                 mask_paths=data_paths['test/masks'],\n",
        "                                 processor=processor)\n",
        "test_dataloader2 = DataLoader(test_dataset2, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4zJG1FCBEwg",
        "outputId": "840bbde5-3019-4451-d41e-31eed4568b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['./Procesado_Balanced/test/images/00015C.jpg']\n",
            "[137.0, 62.0, 189.0, 115.0]\n"
          ]
        }
      ],
      "source": [
        "for n,batch in enumerate(test_dataloader2):\n",
        "    if n == 14:\n",
        "        print(batch['path_name'])\n",
        "        print(batch['input_boxes'][0][0].tolist())\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p96CdH4SXfDa"
      },
      "source": [
        "**Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMATbOipBEwg"
      },
      "outputs": [],
      "source": [
        "METHOD = 'SAM+DETR'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXZ2YhS7BEwh",
        "outputId": "3531168e-3c67-408b-e749-f82fab646436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./Procesado_Balanced/train/images\n",
            "1605\n",
            "./Procesado_Balanced/train/masks\n",
            "1605\n",
            "./Procesado_Balanced/valid/images\n",
            "810\n",
            "./Procesado_Balanced/valid/masks\n",
            "810\n",
            "./Procesado_Balanced/test/images\n",
            "107\n",
            "./Procesado_Balanced/test/masks\n",
            "107\n",
            "Number of training images 1605\n",
            "Number of validation images 810\n",
            "Number of test images 107\n"
          ]
        }
      ],
      "source": [
        "base_dir = './Procesado'+TYPE\n",
        "datasets = ['train', 'valid','test']\n",
        "\n",
        "if METHOD == 'detect':\n",
        "    types = ['DETR', 'masks']\n",
        "else:\n",
        "    types = orig_types\n",
        "\n",
        "# Initialize dictionary for storing image and label paths\n",
        "data_paths = {}\n",
        "\n",
        "# Create directories and print the number of images and masks in each\n",
        "for dataset in datasets:\n",
        "    for data_type in types:\n",
        "        # Construct the directory path\n",
        "        dir_path = os.path.join(base_dir, f'{dataset}/{data_type}')\n",
        "        print(dir_path)\n",
        "\n",
        "        # Find images and labels in the directory\n",
        "        files = sorted(glob.glob(os.path.join(dir_path, \"*.jpg\")))\n",
        "        print(len(files))\n",
        "\n",
        "        # Store the image and label paths in the dictionary\n",
        "        data_paths[f'{dataset}/{data_type}'] = files\n",
        "\n",
        "print('Number of training images', len(data_paths['train/'+types[0]]))\n",
        "print('Number of validation images', len(data_paths['valid/'+types[0]]))\n",
        "print('Number of test images', len(data_paths['test/'+types[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2etTBg3BEwh"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "# create train and validation dataloaders\n",
        "train_dataset = SAM_Pred_Dataset(image_paths=data_paths['train/'+types[0]], mask_paths=data_paths['train/masks'], processor=processor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "val_dataset = SAM_Pred_Dataset(image_paths=data_paths['valid/'+types[0]], mask_paths=data_paths['valid/masks'], processor=processor)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# create test dataloader\n",
        "test_dataset = SAM_Pred_Dataset(image_paths=data_paths['test/'+types[0]], mask_paths=data_paths['test/masks'], processor=processor)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67V3utjZBEwh"
      },
      "outputs": [],
      "source": [
        "#To save the predictions of test\n",
        "try:\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD)\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD+'/test')\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD+'/test/Cáncer')\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD+'/test/Control')\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuxAzy76BEws"
      },
      "outputs": [],
      "source": [
        "orig_masks=[]\n",
        "for r in data_paths['test/masks']:\n",
        "    Yimg = imread(r,as_gray=True)\n",
        "    mask=np.zeros([250, 250])\n",
        "    ind =(Yimg>0.1)\n",
        "    mask[ind]=1\n",
        "\n",
        "    orig_masks.append(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-Fo4uaLBEws",
        "outputId": "96d8e460-173a-4075-9208-39900180b40c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 1024, 1024])\n"
          ]
        }
      ],
      "source": [
        "for batch in test_dataloader:\n",
        "    print(batch['pixel_values'][0].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrY5lqL_BEws",
        "outputId": "0c8fa104-0223-431f-83a2-de52e9cf5382"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "107it [04:14,  2.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST PREDICTIONS COMPLETE\n",
            "*****END*****\n",
            "Tiempo de ejecución:  4.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "inicio=time.time()\n",
        "random.seed(123)\n",
        "\n",
        "if METHOD=='detect':\n",
        "    if TYPE != \"\":\n",
        "        used_model= 'SAM_best_weights2_balanced'\n",
        "    else:\n",
        "        used_model= 'SAM_best_weights2'\n",
        "else:\n",
        "    if TYPE != \"\":\n",
        "        used_model= 'SAM_best_weights1_balanced'\n",
        "    else:\n",
        "        used_model= 'SAM_best_weights1'\n",
        "\n",
        "writepredsdetectDict = [] # Dict Results\n",
        "\n",
        "# Iterate through test images\n",
        "with torch.no_grad():\n",
        "    for indice,batch in tqdm(enumerate(test_dataloader)):\n",
        "        image_name = os.path.basename(str(batch['path_name'][0]))\n",
        "        # forward pass\n",
        "        outputs = modelo(pixel_values=batch[\"pixel_values\"].cuda(),\n",
        "                      input_boxes=batch[\"input_boxes\"].cuda(),\n",
        "                      multimask_output=False)\n",
        "\n",
        "        # compute loss\n",
        "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "        ground_truth_masks = batch[\"ground_truth_mask\"].float().cuda()\n",
        "\n",
        "        # apply sigmoid\n",
        "        medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "        # convert soft mask to hard mask\n",
        "        medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
        "        medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "        medsam_seg2 = cv2.resize(medsam_seg,(250,250))\n",
        "        m_dice = dice_coefficient(medsam_seg2,orig_masks[indice])\n",
        "\n",
        "        if 'C' in image_name:\n",
        "            new_dir='./data'+TYPE+'/'+METHOD+'/test/Cáncer/'\n",
        "            label='Cáncer'\n",
        "        else:\n",
        "            new_dir='./data'+TYPE+'/'+METHOD+'/test/Control/'\n",
        "            label='Control'\n",
        "\n",
        "        Ximg = imread(str(batch['path_name'][0]))\n",
        "        Ximg = cv2.resize(Ximg, (256, 256))\n",
        "\n",
        "        result=cv2.bitwise_and(Ximg,Ximg,mask=medsam_seg)\n",
        "        result=cv2.resize(result,(250,250))\n",
        "        result= cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        cv2.imwrite(new_dir+str(image_name), result)\n",
        "        time.sleep(2)\n",
        "\n",
        "        writepredsdetectDict.append({'modelo':used_model,\n",
        "                                     'imagen':str(image_name),\n",
        "                                     'set':'Test',\n",
        "                                     'clase':label,\n",
        "                                     'true_mask':batch[\"ground_truth_mask\"][0],\n",
        "                                     'predicion':medsam_seg,\n",
        "                                     'DICE': m_dice\n",
        "                                    })\n",
        "print('TEST PREDICTIONS COMPLETE')\n",
        "\n",
        "#------We proceed to store the results in file\n",
        "file_name='resultados.csv'\n",
        "archivo='./data'+TYPE+'/'+METHOD+'/'+file_name\n",
        "if os.path.isfile(archivo):\n",
        "    modo = 'a+'\n",
        "else:\n",
        "    modo = 'w'\n",
        "with open(archivo, modo, newline='') as csvfile:\n",
        "    fieldnames = ['modelo', 'imagen','set', 'clase','true_mask','predicion','DICE']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    if modo=='w':\n",
        "        writer.writeheader()\n",
        "    for d in writepredsdetectDict:\n",
        "        writer.writerow(d)\n",
        "print('*****END*****')\n",
        "print('Tiempo de ejecución: ',round((time.time()-inicio)/60,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POaRBjiKBEwt"
      },
      "outputs": [],
      "source": [
        "#To save the predictions of valid\n",
        "try:\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD+'/valid')\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD+'/valid/Cáncer')\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD+'/valid/Control')\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmmOb1JKBEwt"
      },
      "outputs": [],
      "source": [
        "orig_masks_val=[]\n",
        "for r in data_paths['valid/masks']:\n",
        "    Yimg = imread(r,as_gray=True)\n",
        "    mask=np.zeros([250, 250])\n",
        "    ind =(Yimg>0.1)\n",
        "    mask[ind]=1\n",
        "    orig_masks_val.append(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71LzGAadBEwt",
        "outputId": "1a4e1b34-985c-4d04-afaa-e079d8b7fdee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "810it [32:08,  2.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALID PREDICTIONS COMPLETE\n",
            "*****END*****\n",
            "Tiempo de ejecución:  32.0\n"
          ]
        }
      ],
      "source": [
        "inicio=time.time()\n",
        "\n",
        "random.seed(123)\n",
        "writepredsdetectDict = [] # Dict Results\n",
        "\n",
        "# Iterate through test images\n",
        "with torch.no_grad():\n",
        "    for indice,batch in tqdm(enumerate(val_dataloader)):\n",
        "        image_name = os.path.basename(str(batch['path_name'][0]))\n",
        "        # forward pass\n",
        "        outputs = modelo(pixel_values=batch[\"pixel_values\"].cuda(),\n",
        "                      input_boxes=batch[\"input_boxes\"].cuda(),\n",
        "                      multimask_output=False)\n",
        "\n",
        "        # compute loss\n",
        "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "        ground_truth_masks = batch[\"ground_truth_mask\"].float().cuda()\n",
        "\n",
        "        # apply sigmoid\n",
        "        medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "        # convert soft mask to hard mask\n",
        "        medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
        "        medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "        medsam_seg2 = cv2.resize(medsam_seg,(250,250))\n",
        "        m_dice = dice_coefficient(medsam_seg2,orig_masks_val[indice])\n",
        "\n",
        "        if 'C' in image_name:\n",
        "            new_dir='./data'+TYPE+'/'+METHOD+'/valid/Cáncer/'\n",
        "            label='Cáncer'\n",
        "        else:\n",
        "            new_dir='./data'+TYPE+'/'+METHOD+'/valid/Control/'\n",
        "            label='Control'\n",
        "\n",
        "        Ximg = imread(str(batch['path_name'][0]))\n",
        "        Ximg = cv2.resize(Ximg, (256, 256))\n",
        "\n",
        "        result=cv2.bitwise_and(Ximg,Ximg,mask=medsam_seg)\n",
        "        result=cv2.resize(result,(250,250))\n",
        "        result= cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        cv2.imwrite(new_dir+str(image_name), result)\n",
        "        time.sleep(2)\n",
        "\n",
        "        writepredsdetectDict.append({'modelo':used_model,\n",
        "                                     'imagen':str(image_name),\n",
        "                                     'set':'Valid',\n",
        "                                     'clase':label,\n",
        "                                     'true_mask':batch[\"ground_truth_mask\"][0],\n",
        "                                     'predicion':medsam_seg,\n",
        "                                     'DICE': m_dice\n",
        "                                    })\n",
        "print('VALID PREDICTIONS COMPLETE')\n",
        "\n",
        "#------We proceed to store the results in file\n",
        "file_name='resultados.csv'\n",
        "archivo='./data'+TYPE+'/'+METHOD+'/'+file_name\n",
        "if os.path.isfile(archivo):\n",
        "    modo = 'a+'\n",
        "else:\n",
        "    modo = 'w'\n",
        "with open(archivo, modo, newline='') as csvfile:\n",
        "    fieldnames = ['modelo', 'imagen','set', 'clase','true_mask','predicion','DICE']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    if modo=='w':\n",
        "        writer.writeheader()\n",
        "    for d in writepredsdetectDict:\n",
        "        writer.writerow(d)\n",
        "print('*****END*****')\n",
        "print('Tiempo de ejecución: ',round((time.time()-inicio)/60,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0XOFPo9BEwt"
      },
      "outputs": [],
      "source": [
        "#To save the predictions of train\n",
        "try:\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD+'/train')\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD+'/train/Cáncer')\n",
        "    os.makedirs('./data'+TYPE+'/'+METHOD+'/train/Control')\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpnwTdvFBEwt"
      },
      "outputs": [],
      "source": [
        "orig_masks_train=[]\n",
        "for r in data_paths['train/masks']:\n",
        "    Yimg = imread(r,as_gray=True)\n",
        "    mask=np.zeros([250, 250])\n",
        "    ind =(Yimg>0.1)\n",
        "    mask[ind]=1\n",
        "    orig_masks_train.append(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGLfOTWNBEwt",
        "outputId": "ee18a409-4301-49c1-cd1a-6137df224242"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1605it [1:03:40,  2.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN PREDICTIONS COMPLETE\n",
            "*****END*****\n",
            "Tiempo de ejecución:  64.0\n"
          ]
        }
      ],
      "source": [
        "inicio=time.time()\n",
        "\n",
        "writepredsdetectDict = [] # Dict Results\n",
        "random.seed(123)\n",
        "\n",
        "# Iterate through test images\n",
        "with torch.no_grad():\n",
        "    for indice,batch in tqdm(enumerate(train_dataloader)):\n",
        "        image_name = os.path.basename(str(batch['path_name'][0]))\n",
        "        # forward pass\n",
        "        outputs = modelo(pixel_values=batch[\"pixel_values\"].cuda(),\n",
        "                      input_boxes=batch[\"input_boxes\"].cuda(),\n",
        "                      multimask_output=False)\n",
        "\n",
        "        # compute loss\n",
        "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "        ground_truth_masks = batch[\"ground_truth_mask\"].float().cuda()\n",
        "\n",
        "        # apply sigmoid\n",
        "        medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "        # convert soft mask to hard mask\n",
        "        medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
        "        medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "        medsam_seg2 = cv2.resize(medsam_seg,(250,250))\n",
        "        m_dice = dice_coefficient(medsam_seg2,orig_masks_train[indice])\n",
        "\n",
        "        if 'C' in image_name:\n",
        "            new_dir='./data'+TYPE+'/'+METHOD+'/train/Cáncer/'\n",
        "            label='Cáncer'\n",
        "        else:\n",
        "            new_dir='./data'+TYPE+'/'+METHOD+'/train/Control/'\n",
        "            label='Control'\n",
        "\n",
        "        Ximg = imread(str(batch['path_name'][0]))\n",
        "        Ximg = cv2.resize(Ximg, (256, 256))\n",
        "\n",
        "        result=cv2.bitwise_and(Ximg,Ximg,mask=medsam_seg)\n",
        "        result=cv2.resize(result,(250,250))\n",
        "        result= cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        cv2.imwrite(new_dir+str(image_name), result)\n",
        "        time.sleep(2)\n",
        "\n",
        "        writepredsdetectDict.append({'modelo':used_model,\n",
        "                                     'imagen':str(image_name),\n",
        "                                     'set':'Train',\n",
        "                                     'clase':label,\n",
        "                                     'true_mask':batch[\"ground_truth_mask\"][0],\n",
        "                                     'predicion':medsam_seg,\n",
        "                                     'DICE': m_dice\n",
        "                                    })\n",
        "print('TRAIN PREDICTIONS COMPLETE')\n",
        "\n",
        "#------We proceed to store the results in file\n",
        "file_name='resultados.csv'\n",
        "archivo='./data'+TYPE+'/'+METHOD+'/'+file_name\n",
        "if os.path.isfile(archivo):\n",
        "    modo = 'a+'\n",
        "else:\n",
        "    modo = 'w'\n",
        "with open(archivo, modo, newline='') as csvfile:\n",
        "    fieldnames = ['modelo', 'imagen','set', 'clase','true_mask','predicion','DICE']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    if modo=='w':\n",
        "        writer.writeheader()\n",
        "    for d in writepredsdetectDict:\n",
        "        writer.writerow(d)\n",
        "print('*****END*****')\n",
        "print('Tiempo de ejecución: ',round((time.time()-inicio)/60,0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14caad206f26469aa698ad9e8812c426": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67e85c24561c4e9cb987b9016fb75970",
              "IPY_MODEL_d861f1f866eb4f2893dd597b12e96f58",
              "IPY_MODEL_9d181729481b4f6ea82c9652112979e2"
            ],
            "layout": "IPY_MODEL_84a6d4e6fb524f13b1b3e48ac0f815de"
          }
        },
        "2fbcd6ca7dae441397182cb12aae7aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67e85c24561c4e9cb987b9016fb75970": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9281f046be544713a828e14c48cc9267",
            "placeholder": "​",
            "style": "IPY_MODEL_97ade329f27e4116b7ea4aeb0550a8e3",
            "value": "Downloading (…)rocessor_config.json: 100%"
          }
        },
        "84a6d4e6fb524f13b1b3e48ac0f815de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9281f046be544713a828e14c48cc9267": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97ade329f27e4116b7ea4aeb0550a8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d181729481b4f6ea82c9652112979e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e80735ff47447d39d48d2d6da0a3203",
            "placeholder": "​",
            "style": "IPY_MODEL_c0cd6ba97e324e6c8fc254dad47d780c",
            "value": " 466/466 [00:00&lt;00:00, 19.8kB/s]"
          }
        },
        "9e80735ff47447d39d48d2d6da0a3203": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af0e1ea7607d42e686758a234cf1ed06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0cd6ba97e324e6c8fc254dad47d780c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d861f1f866eb4f2893dd597b12e96f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fbcd6ca7dae441397182cb12aae7aaa",
            "max": 466,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af0e1ea7607d42e686758a234cf1ed06",
            "value": 466
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}